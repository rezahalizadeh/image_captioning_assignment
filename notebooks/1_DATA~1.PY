# -*- coding: utf-8 -*-
"""1_Data_Exploration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bXOPnZQn4BrHnFq92O77abPtcSIGipa-

# Image Captioning - Data Exploration

This notebook explores the Flickr8k dataset for the image captioning task. We will:

1. Download and prepare the dataset
2. Explore the images and captions
3. Analyze the distribution of caption lengths
4. Examine the vocabulary
5. Visualize some sample images with their captions
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import nltk
nltk.download('punkt_tab')
from collections import Counter
from tqdm import tqdm
import re
import string
import seaborn as sns

# Add project root to path
sys.path.append('..')

# Import project modules
from data.download_flickr import download_flickr8k
from utils.vocabulary import Vocabulary, build_vocab_from_captions

"""## 1. Download and Prepare the Dataset"""

# Download and prepare the Flickr8k dataset
data_dir = '../data'
paths = download_flickr8k(data_dir)

# Load the captions
captions_path = os.path.join(paths['processed_path'], 'captions.csv')
captions_df = pd.read_csv(captions_path)

# Display the first few rows
captions_df.head()

"""## 2. Explore the Images and Captions"""

# Count unique images and total captions
unique_images = captions_df['image'].nunique()
total_captions = len(captions_df)

print(f"Total number of images: {unique_images}")
print(f"Total number of captions: {total_captions}")
print(f"Average captions per image: {total_captions / unique_images:.2f}")

# Display a random sample of images with their captions
# TODO: Implement the function to display images with their captions
# This function should:
# 1. Take a DataFrame of captions, path to images directory, and number of samples
# 2. Get random image samples
# 3. Create a figure with subplots
# 4. For each image:
#    a. Load and display the image
#    b. Find all captions for that image
#    c. Add captions as a formatted title
def display_images_with_captions(captions_df, images_dir, num_samples=5):
  # Get unique images
  unique_image_df = captions_df.drop_duplicates(subset=['image'])

  # Your implementation here
  sample_images = unique_image_df.sample(n=num_samples, random_state=42)

  # Create a figure with subplots
  plt.figure(figsize=(15, num_samples * 3))

  for idx, row in enumerate(sample_images.itertuples()):
    # Load and display the image
    img_path = os.path.join(images_dir, row.image)
    image = Image.open(img_path).convert("RGB")

    # Find all captions for that image
    image_captions = captions_df[captions_df['image'] == row.image]['caption'].tolist()
    caption_text = "\n".join(image_captions)

    # Add captions as a formatted title
    plt.subplot(num_samples, 1, idx + 1)
    plt.imshow(image)
    plt.axis('off')
    plt.title(caption_text, fontsize=10)

  plt.tight_layout()
  plt.show()

# Display some sample images with their captions
images_dir = os.path.join(paths['processed_path'], 'images')
display_images_with_captions(captions_df, images_dir)

"""## 3. Analyze Caption Lengths"""

# Tokenize captions
# TODO: Implement the tokenize function for caption text
# This function should:
# 1. Convert text to lowercase
# 2. Remove punctuation using regex
# 3. Split text into tokens using NLTK's word_tokenize
# 4. Return the list of tokens
def tokenize(text):
  # Your implementation here

  # Convert text to lowercase
  text = text.lower()

  # Remove punctuation using regex
  text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)

  # Split text into tokens using NLTK's word_tokenize
  tokens = nltk.word_tokenize(text)

  # Return the list of tokens
  return tokens

# Calculate caption lengths
captions_df['tokens'] = captions_df['caption'].apply(tokenize)
captions_df['length'] = captions_df['tokens'].apply(len)

# Display statistics
caption_lengths = captions_df['length']
print(f"Min length: {caption_lengths.min()}")
print(f"Max length: {caption_lengths.max()}")
print(f"Mean length: {caption_lengths.mean():.2f}")
print(f"Median length: {caption_lengths.median()}")
print(f"90th percentile length: {caption_lengths.quantile(0.9)}")
print(f"95th percentile length: {caption_lengths.quantile(0.95)}")

# Plot the distribution of caption lengths
plt.figure(figsize=(10, 6))
sns.histplot(caption_lengths, bins=20, kde=True)
plt.axvline(x=caption_lengths.mean(), color='r', linestyle='--', label=f'Mean: {caption_lengths.mean():.2f}')
plt.axvline(x=caption_lengths.median(), color='g', linestyle='--', label=f'Median: {caption_lengths.median()}')
plt.axvline(x=caption_lengths.quantile(0.95), color='b', linestyle='--', label=f'95th percentile: {caption_lengths.quantile(0.95)}')
plt.title('Distribution of Caption Lengths')
plt.xlabel('Number of Words')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

"""## 4. Examine the Vocabulary"""

# Count word frequencies
all_tokens = [token for tokens in captions_df['tokens'] for token in tokens]
word_freq = Counter(all_tokens)

# Display statistics
print(f"Total vocabulary size: {len(word_freq)}")
print(f"Number of words appearing only once: {sum(1 for count in word_freq.values() if count == 1)}")

# Plot the most common words
top_n = 30
most_common = word_freq.most_common(top_n)
words, counts = zip(*most_common)

plt.figure(figsize=(12, 8))
sns.barplot(x=list(counts), y=list(words))
plt.title(f'Top {top_n} Most Common Words')
plt.xlabel('Frequency')
plt.grid(True, alpha=0.3)
plt.show()

# Calculate vocabulary coverage with different frequency thresholds
# TODO: Calculate vocabulary coverage for different word frequency thresholds
# For each threshold:
# 1. Find words that appear at least 'threshold' times
# 2. Calculate vocabulary size (number of unique words above threshold)
# 3. Calculate what percentage of all tokens are covered by this vocabulary
# 4. Store results for plotting
thresholds = [1, 2, 3, 5, 10]
coverage = []

for threshold in thresholds:
  # Your implementation here

  # Find words that appear at least 'threshold' times
  filtered_words = [word for word, count in word_freq.items() if count >= threshold]

  # Calculate vocabulary size (number of unique words above threshold)
  vocab_size = len(filtered_words)

  # Calculate what percentage of all tokens are covered by this vocabulary
  total_tokens = sum(word_freq.values())
  covered_tokens = sum(count for word, count in word_freq.items() if word in filtered_words)
  coverage_pct = (covered_tokens / total_tokens) * 100

  # Store results for plotting
  coverage.append((threshold, vocab_size, coverage_pct))

  print(f"Threshold: {threshold}, Vocabulary size: {vocab_size}, Coverage: {coverage_pct:.2f}%")

# Plot vocabulary coverage
thresholds, vocab_sizes, coverages = zip(*coverage)

fig, ax1 = plt.subplots(figsize=(10, 6))

# Plot vocabulary size
color = 'tab:blue'
ax1.set_xlabel('Frequency Threshold')
ax1.set_ylabel('Vocabulary Size', color=color)
ax1.plot(thresholds, vocab_sizes, 'o-', color=color)
ax1.tick_params(axis='y', labelcolor=color)

# Create second y-axis for coverage
ax2 = ax1.twinx()
color = 'tab:red'
ax2.set_ylabel('Coverage (%)', color=color)
ax2.plot(thresholds, coverages, 's-', color=color)
ax2.tick_params(axis='y', labelcolor=color)

plt.title('Vocabulary Size and Coverage vs. Frequency Threshold')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

"""## 5. Build and Save the Vocabulary"""

# Build and save the vocabulary
freq_threshold = 5  # Words appearing less than 5 times are considered rare
vocab = build_vocab_from_captions(captions_path, paths['processed_path'], freq_threshold=freq_threshold)

print(f"Built vocabulary with {len(vocab)} words")

"""## 6. Explore Data Splits"""

# Load data splits
train_df = pd.read_csv(os.path.join(paths['processed_path'], 'train_captions.csv'))
val_df = pd.read_csv(os.path.join(paths['processed_path'], 'val_captions.csv'))
test_df = pd.read_csv(os.path.join(paths['processed_path'], 'test_captions.csv'))

# Display statistics
print(f"Training set: {train_df['image'].nunique()} images, {len(train_df)} captions")
print(f"Validation set: {val_df['image'].nunique()} images, {len(val_df)} captions")
print(f"Test set: {test_df['image'].nunique()} images, {len(test_df)} captions")

"""## 7. Check Image Dimensions"""

# Check image dimensions for a sample of images
# TODO: Implement a function to analyze image dimensions in the dataset
# This function should:
# 1. Get a sample of image files from the directory
# 2. Load each image and extract its dimensions
# 3. Return a DataFrame with width and height columns for analysis
def check_image_dimensions(images_dir, num_samples=100):
  # Your implementation here
  # Get a sample of image files from the directory
  image_files = os.listdir(images_dir)
  sampled_files = np.random.choice(image_files, size=num_samples, replace=False)

  # Load each image and extract its dimensions
  dimensions = []
  for img_file in tqdm(sampled_files, desc="Analyzing image dimensions"):
    img_path = os.path.join(images_dir, img_file)
    with Image.open(img_path) as img:
      width, height = img.size
      dimensions.append({'width': width, 'height': height})

  # Return a DataFrame with width and height columns for analysis
  dim_df = pd.DataFrame(dimensions)

  return dim_df


# Check dimensions
image_dimensions = check_image_dimensions(images_dir)

# Display statistics
print("Image dimension statistics:")
print(image_dimensions.describe())

# Plot image dimensions
plt.figure(figsize=(10, 8))
plt.scatter(image_dimensions['width'], image_dimensions['height'], alpha=0.5)
plt.title('Image Dimensions')
plt.xlabel('Width (pixels)')
plt.ylabel('Height (pixels)')
plt.grid(True, alpha=0.3)
plt.axis('equal')
plt.show()

"""## Summary

In this notebook, we have explored the Flickr8k dataset for image captioning. We have:

1. Downloaded and prepared the dataset
2. Explored the images and captions
3. Analyzed the distribution of caption lengths
4. Examined the vocabulary and its coverage
5. Built and saved the vocabulary
6. Explored the data splits
7. Checked image dimensions

This exploration gives us a good understanding of the dataset and helps us make informed decisions when designing our image captioning model.
"""